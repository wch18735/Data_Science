{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"lab07_simple_deep_learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uCdyHxGO7ux8"},"source":["# Before we go for M-4 labs\n","- from \"모두의 딥러닝\""]},{"cell_type":"markdown","metadata":{"id":"70crPSVy7uyB"},"source":["## 12. multinomial classification\n","- iris data\n","- one-hot encoding\n","- softmax\n","- F-fold cross validation"]},{"cell_type":"code","metadata":{"id":"5axxcoz_7uyF"},"source":["import tensorflow as tf\n","from keras.models import Sequential, load_model\n","from keras.layers.core import Dense\n","from keras.utils import np_utils\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1LZ1bMH7uyT","executionInfo":{"status":"ok","timestamp":1605354086315,"user_tz":-540,"elapsed":905,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"549988a0-ede0-4d14-d79b-45ba120f1649","colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["np.random.seed(17)\n","iris_data = sns.load_dataset(\"iris\")\n","iris_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width species\n","0           5.1          3.5           1.4          0.2  setosa\n","1           4.9          3.0           1.4          0.2  setosa\n","2           4.7          3.2           1.3          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","4           5.0          3.6           1.4          0.2  setosa"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"joA66Ufe7uya"},"source":["dataset = iris_data.values\n","X = dataset[:,0:4].astype('float')\n","Y_obj = dataset[:,4]\n","\n","Y = LabelEncoder().fit_transform(Y_obj)\n","Y_encoded = np_utils.to_categorical(Y)   #  one-hot encoding\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4bN4kgX7uyg","executionInfo":{"status":"ok","timestamp":1605354172692,"user_tz":-540,"elapsed":701,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"dbba353b-33c7-49f7-b58e-f17374a22ac5","colab":{"base_uri":"https://localhost:8080/"}},"source":["# model and compile & train & predict(evaluate)\n","model = Sequential()\n","# model.add(Dense(16,  input_dim=4, activation='relu'))\n","model.add(Dense(16,  input_shape=(4,), activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_12\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_24 (Dense)             (None, 16)                80        \n","_________________________________________________________________\n","dense_25 (Dense)             (None, 3)                 51        \n","=================================================================\n","Total params: 131\n","Trainable params: 131\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vo4_n9EY7uym","executionInfo":{"status":"ok","timestamp":1605354180766,"user_tz":-540,"elapsed":782,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"447c7895-9fce-42d3-8d42-0b83b957b2b6","colab":{"base_uri":"https://localhost:8080/"}},"source":["X_train.shape, Y_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((120, 4), (120, 3))"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"qVA55goD7uyr","executionInfo":{"status":"ok","timestamp":1605354186584,"user_tz":-540,"elapsed":3512,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"f8d31d80-a6e1-4f5b-c586-c648a92a2459","colab":{"base_uri":"https://localhost:8080/"}},"source":["model.compile(loss='categorical_crossentropy',\n","            optimizer='adam',\n","            metrics=['accuracy'])\n","\n","history = model.fit(X_train, Y_train, epochs=50, batch_size=5)\n","model.save('iris_model.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","24/24 [==============================] - 0s 2ms/step - loss: 1.3312 - accuracy: 0.3333\n","Epoch 2/50\n","24/24 [==============================] - 0s 2ms/step - loss: 1.0453 - accuracy: 0.5667\n","Epoch 3/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.8886 - accuracy: 0.7083\n","Epoch 4/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.7709 - accuracy: 0.7000\n","Epoch 5/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.6833\n","Epoch 6/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.6317 - accuracy: 0.6500\n","Epoch 7/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.6750\n","Epoch 8/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7000\n","Epoch 9/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.6917\n","Epoch 10/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.6917\n","Epoch 11/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7083\n","Epoch 12/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7000\n","Epoch 13/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7167\n","Epoch 14/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7333\n","Epoch 15/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7917\n","Epoch 16/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8250\n","Epoch 17/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.7250\n","Epoch 18/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8750\n","Epoch 19/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.9000\n","Epoch 20/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.9083\n","Epoch 21/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.9167\n","Epoch 22/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.9167\n","Epoch 23/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.9083\n","Epoch 24/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.9333\n","Epoch 25/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.9250\n","Epoch 26/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.9500\n","Epoch 27/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.9583\n","Epoch 28/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.9333\n","Epoch 29/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.9500\n","Epoch 30/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.9333\n","Epoch 31/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.9583\n","Epoch 32/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.9583\n","Epoch 33/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9583\n","Epoch 34/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9667\n","Epoch 35/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.9667\n","Epoch 36/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9583\n","Epoch 37/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9667\n","Epoch 38/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.9667\n","Epoch 39/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9417\n","Epoch 40/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9500\n","Epoch 41/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9750\n","Epoch 42/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9750\n","Epoch 43/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9750\n","Epoch 44/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9583\n","Epoch 45/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9667\n","Epoch 46/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9667\n","Epoch 47/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9750\n","Epoch 48/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9667\n","Epoch 49/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9750\n","Epoch 50/50\n","24/24 [==============================] - 0s 2ms/step - loss: 0.1934 - accuracy: 0.9667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wA6psJlU7uyz","executionInfo":{"status":"ok","timestamp":1605354205307,"user_tz":-540,"elapsed":1077,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"98706951-143e-4c48-9fa6-cd77d8fca0fb","colab":{"base_uri":"https://localhost:8080/"}},"source":["from keras.models import load_model\n","del model   # delete existing models in memory\n","model = load_model('iris_model.h5')  # newly call the model\n","print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f83fa0b6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 0s 4ms/step - loss: 0.1991 - accuracy: 1.0000\n","\n"," Accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TfJAAPbbYsLB","executionInfo":{"status":"ok","timestamp":1605354214054,"user_tz":-540,"elapsed":697,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"e591b2d1-27df-4445-ea66-f1d45f27cc4f","colab":{"base_uri":"https://localhost:8080/"}},"source":["X.shape, Y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((150, 4), (150,))"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"KitK6yZf7uzE"},"source":["## for K-fold validation"]},{"cell_type":"code","metadata":{"id":"yuLk6qSk7uzF","executionInfo":{"status":"ok","timestamp":1605354432706,"user_tz":-540,"elapsed":10754,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"3296f74b-c8fb-4074-eb64-2750584ac092","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","n_fold = 5\n","skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=17)\n","\n","cvscores = []\n","for train_index, test_index in skf.split(X, Y):\n","    model = Sequential()\n","    model.add(Dense(16,  input_dim=4, activation='relu'))\n","    model.add(Dense(3, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam', \n","                  metrics=['accuracy'])\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = Y[train_index], Y[test_index]\n","    y_train_en = np_utils.to_categorical(y_train)\n","    y_test_enc = np_utils.to_categorical(y_test)\n","    \n","    model.fit(X_train, y_train_en, epochs=50, batch_size=5, verbose=0)\n","    score = model.evaluate(X_test, y_test_enc, verbose=0)\n","    cvscores.append(score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f83fa15eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f840c2e6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f840c24e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f840c1d6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f83edc34b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V6WIi1FtYL-Z","executionInfo":{"status":"ok","timestamp":1605354764907,"user_tz":-540,"elapsed":814,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"c5dc2a21-539e-48c1-bc6f-d13b0788e835","colab":{"base_uri":"https://localhost:8080/"}},"source":["model.metrics_names, cvscores"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['loss', 'accuracy'],\n"," [[0.20477159321308136, 0.9666666388511658],\n","  [0.3510998785495758, 0.8999999761581421],\n","  [0.26057079434394836, 0.9666666388511658],\n","  [0.2964290380477905, 0.9666666388511658],\n","  [0.2828664481639862, 0.9666666388511658]])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"UzbpqKRk7uzT"},"source":["## 14. Callback - check point and early stopping"]},{"cell_type":"code","metadata":{"id":"gafjD4O27uzU"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bj5eEiks7uzY","executionInfo":{"status":"ok","timestamp":1605354928821,"user_tz":-540,"elapsed":745,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"69cace5b-25f0-47c1-a8d9-104a0bfbf4a2","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_all = pd.read_csv('wine.csv', header=None)\n","df = df_all.sample(frac=0.2)  # get only 20% of dataset\n","df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1299, 13)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"TY5sKbdJnQBQ","executionInfo":{"status":"ok","timestamp":1605354987481,"user_tz":-540,"elapsed":688,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"39b06494-c0e3-41cc-eb16-dbccd9408f39","colab":{"base_uri":"https://localhost:8080/","height":78}},"source":["df.head(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5510</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.82</td>\n","      <td>1.3</td>\n","      <td>0.149</td>\n","      <td>70.0</td>\n","      <td>109.0</td>\n","      <td>0.99304</td>\n","      <td>2.93</td>\n","      <td>0.42</td>\n","      <td>9.2</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       0     1     2    3      4     5   ...       7     8     9    10  11  12\n","5510  7.2  0.23  0.82  1.3  0.149  70.0  ...  0.99304  2.93  0.42  9.2   6   0\n","\n","[1 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"dV8EsphhnWsH","executionInfo":{"status":"ok","timestamp":1605355044841,"user_tz":-540,"elapsed":685,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"0ab6966d-c834-454e-fe34-4e72248a765e","colab":{"base_uri":"https://localhost:8080/"}},"source":["df[12].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    980\n","1    319\n","Name: 12, dtype: int64"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"Yd7tFmGT7uzd"},"source":["dataset = df.values\n","X, y = dataset[:,0:12], dataset[:,12]\n","\n","model = Sequential()\n","model.add(Dense(30,  input_dim=12, activation='relu'))\n","model.add(Dense(12, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","          optimizer='adam',\n","          metrics=['accuracy'])\n","\n","# 모델 저장 폴더 만들기\n","MODEL_DIR = './model/'\n","if not os.path.exists(MODEL_DIR):\n","    os.mkdir(MODEL_DIR)\n","\n","modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","\n","# 모델 업데이트 및 저장 (epoch 마다)\n","checkpointer = ModelCheckpoint(filepath=modelpath, \n","                               monitor='val_loss', \n","                               verbose=1, \n","                               save_best_only=True)  # record only when imrpoved\n","\n","# 테스트 오차가 줄지 않으면 학습 자동 중단 설정 (모니터할 값 저장)\n","early_stopping_callback = EarlyStopping(monitor='val_loss', \n","                                        patience=100) # 좋아지지 않아도 몇 번까지 기다릴것인지"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"--_bUDPb7uzp","executionInfo":{"status":"ok","timestamp":1605355318596,"user_tz":-540,"elapsed":15884,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"72f95ebd-7d1b-46f6-a328-a9cbb05b5f4e","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["y_loss, y_acc, y_vloss, y_vacc = [], [], [], []\n","\n","history = model.fit(X, y, validation_split=0.2, \n","                    epochs=2000, batch_size=100, verbose=0, \n","                    callbacks=[early_stopping_callback,checkpointer])\n","y_loss = history.history['loss']\n","y_acc = history.history['accuracy']\n","y_vloss = history.history['val_loss']\n","y_vacc = history.history['val_accuracy']\n","x_len = np.arange(len(y_acc))\n","plt.ylim(0.,1.)\n","plt.title(\"Traing\")\n","plt.plot(x_len, y_loss, \"o\", c=\"r\", markersize=3)\n","plt.plot(x_len, y_acc, \"o\", c=\"b\", markersize=3)\n","plt.show()\n","plt.title(\"Validation\")\n","plt.ylim(0.,1.)\n","plt.plot(x_len, y_vloss, \"o\", c=\"r\", markersize=3)\n","plt.plot(x_len, y_vacc, \"o\", c=\"b\", markersize=3)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f83ec9f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","\n","Epoch 00001: val_loss improved from inf to 1.79359, saving model to ./model/01-1.7936.hdf5\n","\n","Epoch 00002: val_loss improved from 1.79359 to 0.49681, saving model to ./model/02-0.4968.hdf5\n","\n","Epoch 00003: val_loss improved from 0.49681 to 0.30308, saving model to ./model/03-0.3031.hdf5\n","\n","Epoch 00004: val_loss did not improve from 0.30308\n","\n","Epoch 00005: val_loss improved from 0.30308 to 0.27554, saving model to ./model/05-0.2755.hdf5\n","\n","Epoch 00006: val_loss did not improve from 0.27554\n","\n","Epoch 00007: val_loss improved from 0.27554 to 0.26100, saving model to ./model/07-0.2610.hdf5\n","\n","Epoch 00008: val_loss improved from 0.26100 to 0.25449, saving model to ./model/08-0.2545.hdf5\n","\n","Epoch 00009: val_loss did not improve from 0.25449\n","\n","Epoch 00010: val_loss improved from 0.25449 to 0.23732, saving model to ./model/10-0.2373.hdf5\n","\n","Epoch 00011: val_loss did not improve from 0.23732\n","\n","Epoch 00012: val_loss improved from 0.23732 to 0.22651, saving model to ./model/12-0.2265.hdf5\n","\n","Epoch 00013: val_loss improved from 0.22651 to 0.21698, saving model to ./model/13-0.2170.hdf5\n","\n","Epoch 00014: val_loss did not improve from 0.21698\n","\n","Epoch 00015: val_loss improved from 0.21698 to 0.21000, saving model to ./model/15-0.2100.hdf5\n","\n","Epoch 00016: val_loss improved from 0.21000 to 0.20947, saving model to ./model/16-0.2095.hdf5\n","\n","Epoch 00017: val_loss improved from 0.20947 to 0.20627, saving model to ./model/17-0.2063.hdf5\n","\n","Epoch 00018: val_loss improved from 0.20627 to 0.19948, saving model to ./model/18-0.1995.hdf5\n","\n","Epoch 00019: val_loss improved from 0.19948 to 0.19312, saving model to ./model/19-0.1931.hdf5\n","\n","Epoch 00020: val_loss did not improve from 0.19312\n","\n","Epoch 00021: val_loss improved from 0.19312 to 0.18803, saving model to ./model/21-0.1880.hdf5\n","\n","Epoch 00022: val_loss did not improve from 0.18803\n","\n","Epoch 00023: val_loss improved from 0.18803 to 0.18749, saving model to ./model/23-0.1875.hdf5\n","\n","Epoch 00024: val_loss improved from 0.18749 to 0.17687, saving model to ./model/24-0.1769.hdf5\n","\n","Epoch 00025: val_loss did not improve from 0.17687\n","\n","Epoch 00026: val_loss did not improve from 0.17687\n","\n","Epoch 00027: val_loss did not improve from 0.17687\n","\n","Epoch 00028: val_loss improved from 0.17687 to 0.16865, saving model to ./model/28-0.1686.hdf5\n","\n","Epoch 00029: val_loss did not improve from 0.16865\n","\n","Epoch 00030: val_loss did not improve from 0.16865\n","\n","Epoch 00031: val_loss did not improve from 0.16865\n","\n","Epoch 00032: val_loss did not improve from 0.16865\n","\n","Epoch 00033: val_loss did not improve from 0.16865\n","\n","Epoch 00034: val_loss improved from 0.16865 to 0.16713, saving model to ./model/34-0.1671.hdf5\n","\n","Epoch 00035: val_loss did not improve from 0.16713\n","\n","Epoch 00036: val_loss improved from 0.16713 to 0.16371, saving model to ./model/36-0.1637.hdf5\n","\n","Epoch 00037: val_loss improved from 0.16371 to 0.15497, saving model to ./model/37-0.1550.hdf5\n","\n","Epoch 00038: val_loss did not improve from 0.15497\n","\n","Epoch 00039: val_loss improved from 0.15497 to 0.15339, saving model to ./model/39-0.1534.hdf5\n","\n","Epoch 00040: val_loss did not improve from 0.15339\n","\n","Epoch 00041: val_loss did not improve from 0.15339\n","\n","Epoch 00042: val_loss did not improve from 0.15339\n","\n","Epoch 00043: val_loss improved from 0.15339 to 0.14659, saving model to ./model/43-0.1466.hdf5\n","\n","Epoch 00044: val_loss did not improve from 0.14659\n","\n","Epoch 00045: val_loss did not improve from 0.14659\n","\n","Epoch 00046: val_loss did not improve from 0.14659\n","\n","Epoch 00047: val_loss did not improve from 0.14659\n","\n","Epoch 00048: val_loss improved from 0.14659 to 0.14218, saving model to ./model/48-0.1422.hdf5\n","\n","Epoch 00049: val_loss did not improve from 0.14218\n","\n","Epoch 00050: val_loss did not improve from 0.14218\n","\n","Epoch 00051: val_loss improved from 0.14218 to 0.13885, saving model to ./model/51-0.1389.hdf5\n","\n","Epoch 00052: val_loss did not improve from 0.13885\n","\n","Epoch 00053: val_loss improved from 0.13885 to 0.13650, saving model to ./model/53-0.1365.hdf5\n","\n","Epoch 00054: val_loss improved from 0.13650 to 0.13589, saving model to ./model/54-0.1359.hdf5\n","\n","Epoch 00055: val_loss did not improve from 0.13589\n","\n","Epoch 00056: val_loss did not improve from 0.13589\n","\n","Epoch 00057: val_loss improved from 0.13589 to 0.13258, saving model to ./model/57-0.1326.hdf5\n","\n","Epoch 00058: val_loss improved from 0.13258 to 0.13032, saving model to ./model/58-0.1303.hdf5\n","\n","Epoch 00059: val_loss did not improve from 0.13032\n","\n","Epoch 00060: val_loss did not improve from 0.13032\n","\n","Epoch 00061: val_loss did not improve from 0.13032\n","\n","Epoch 00062: val_loss did not improve from 0.13032\n","\n","Epoch 00063: val_loss improved from 0.13032 to 0.12733, saving model to ./model/63-0.1273.hdf5\n","\n","Epoch 00064: val_loss did not improve from 0.12733\n","\n","Epoch 00065: val_loss did not improve from 0.12733\n","\n","Epoch 00066: val_loss did not improve from 0.12733\n","\n","Epoch 00067: val_loss improved from 0.12733 to 0.12644, saving model to ./model/67-0.1264.hdf5\n","\n","Epoch 00068: val_loss improved from 0.12644 to 0.12460, saving model to ./model/68-0.1246.hdf5\n","\n","Epoch 00069: val_loss did not improve from 0.12460\n","\n","Epoch 00070: val_loss did not improve from 0.12460\n","\n","Epoch 00071: val_loss did not improve from 0.12460\n","\n","Epoch 00072: val_loss did not improve from 0.12460\n","\n","Epoch 00073: val_loss improved from 0.12460 to 0.12383, saving model to ./model/73-0.1238.hdf5\n","\n","Epoch 00074: val_loss did not improve from 0.12383\n","\n","Epoch 00075: val_loss did not improve from 0.12383\n","\n","Epoch 00076: val_loss did not improve from 0.12383\n","\n","Epoch 00077: val_loss did not improve from 0.12383\n","\n","Epoch 00078: val_loss did not improve from 0.12383\n","\n","Epoch 00079: val_loss improved from 0.12383 to 0.12279, saving model to ./model/79-0.1228.hdf5\n","\n","Epoch 00080: val_loss improved from 0.12279 to 0.11748, saving model to ./model/80-0.1175.hdf5\n","\n","Epoch 00081: val_loss did not improve from 0.11748\n","\n","Epoch 00082: val_loss did not improve from 0.11748\n","\n","Epoch 00083: val_loss did not improve from 0.11748\n","\n","Epoch 00084: val_loss did not improve from 0.11748\n","\n","Epoch 00085: val_loss did not improve from 0.11748\n","\n","Epoch 00086: val_loss improved from 0.11748 to 0.11623, saving model to ./model/86-0.1162.hdf5\n","\n","Epoch 00087: val_loss did not improve from 0.11623\n","\n","Epoch 00088: val_loss did not improve from 0.11623\n","\n","Epoch 00089: val_loss did not improve from 0.11623\n","\n","Epoch 00090: val_loss did not improve from 0.11623\n","\n","Epoch 00091: val_loss did not improve from 0.11623\n","\n","Epoch 00092: val_loss did not improve from 0.11623\n","\n","Epoch 00093: val_loss did not improve from 0.11623\n","\n","Epoch 00094: val_loss did not improve from 0.11623\n","\n","Epoch 00095: val_loss did not improve from 0.11623\n","\n","Epoch 00096: val_loss did not improve from 0.11623\n","\n","Epoch 00097: val_loss did not improve from 0.11623\n","\n","Epoch 00098: val_loss did not improve from 0.11623\n","\n","Epoch 00099: val_loss did not improve from 0.11623\n","\n","Epoch 00100: val_loss improved from 0.11623 to 0.11488, saving model to ./model/100-0.1149.hdf5\n","\n","Epoch 00101: val_loss did not improve from 0.11488\n","\n","Epoch 00102: val_loss improved from 0.11488 to 0.11376, saving model to ./model/102-0.1138.hdf5\n","\n","Epoch 00103: val_loss did not improve from 0.11376\n","\n","Epoch 00104: val_loss did not improve from 0.11376\n","\n","Epoch 00105: val_loss did not improve from 0.11376\n","\n","Epoch 00106: val_loss did not improve from 0.11376\n","\n","Epoch 00107: val_loss improved from 0.11376 to 0.11170, saving model to ./model/107-0.1117.hdf5\n","\n","Epoch 00108: val_loss improved from 0.11170 to 0.10934, saving model to ./model/108-0.1093.hdf5\n","\n","Epoch 00109: val_loss did not improve from 0.10934\n","\n","Epoch 00110: val_loss did not improve from 0.10934\n","\n","Epoch 00111: val_loss did not improve from 0.10934\n","\n","Epoch 00112: val_loss did not improve from 0.10934\n","\n","Epoch 00113: val_loss did not improve from 0.10934\n","\n","Epoch 00114: val_loss did not improve from 0.10934\n","\n","Epoch 00115: val_loss did not improve from 0.10934\n","\n","Epoch 00116: val_loss did not improve from 0.10934\n","\n","Epoch 00117: val_loss did not improve from 0.10934\n","\n","Epoch 00118: val_loss did not improve from 0.10934\n","\n","Epoch 00119: val_loss improved from 0.10934 to 0.10790, saving model to ./model/119-0.1079.hdf5\n","\n","Epoch 00120: val_loss did not improve from 0.10790\n","\n","Epoch 00121: val_loss did not improve from 0.10790\n","\n","Epoch 00122: val_loss did not improve from 0.10790\n","\n","Epoch 00123: val_loss did not improve from 0.10790\n","\n","Epoch 00124: val_loss did not improve from 0.10790\n","\n","Epoch 00125: val_loss did not improve from 0.10790\n","\n","Epoch 00126: val_loss did not improve from 0.10790\n","\n","Epoch 00127: val_loss did not improve from 0.10790\n","\n","Epoch 00128: val_loss did not improve from 0.10790\n","\n","Epoch 00129: val_loss did not improve from 0.10790\n","\n","Epoch 00130: val_loss improved from 0.10790 to 0.10756, saving model to ./model/130-0.1076.hdf5\n","\n","Epoch 00131: val_loss did not improve from 0.10756\n","\n","Epoch 00132: val_loss did not improve from 0.10756\n","\n","Epoch 00133: val_loss did not improve from 0.10756\n","\n","Epoch 00134: val_loss did not improve from 0.10756\n","\n","Epoch 00135: val_loss did not improve from 0.10756\n","\n","Epoch 00136: val_loss did not improve from 0.10756\n","\n","Epoch 00137: val_loss did not improve from 0.10756\n","\n","Epoch 00138: val_loss did not improve from 0.10756\n","\n","Epoch 00139: val_loss did not improve from 0.10756\n","\n","Epoch 00140: val_loss did not improve from 0.10756\n","\n","Epoch 00141: val_loss did not improve from 0.10756\n","\n","Epoch 00142: val_loss did not improve from 0.10756\n","\n","Epoch 00143: val_loss did not improve from 0.10756\n","\n","Epoch 00144: val_loss did not improve from 0.10756\n","\n","Epoch 00145: val_loss did not improve from 0.10756\n","\n","Epoch 00146: val_loss did not improve from 0.10756\n","\n","Epoch 00147: val_loss did not improve from 0.10756\n","\n","Epoch 00148: val_loss did not improve from 0.10756\n","\n","Epoch 00149: val_loss did not improve from 0.10756\n","\n","Epoch 00150: val_loss did not improve from 0.10756\n","\n","Epoch 00151: val_loss did not improve from 0.10756\n","\n","Epoch 00152: val_loss did not improve from 0.10756\n","\n","Epoch 00153: val_loss did not improve from 0.10756\n","\n","Epoch 00154: val_loss did not improve from 0.10756\n","\n","Epoch 00155: val_loss did not improve from 0.10756\n","\n","Epoch 00156: val_loss improved from 0.10756 to 0.10715, saving model to ./model/156-0.1072.hdf5\n","\n","Epoch 00157: val_loss did not improve from 0.10715\n","\n","Epoch 00158: val_loss improved from 0.10715 to 0.10516, saving model to ./model/158-0.1052.hdf5\n","\n","Epoch 00159: val_loss did not improve from 0.10516\n","\n","Epoch 00160: val_loss did not improve from 0.10516\n","\n","Epoch 00161: val_loss did not improve from 0.10516\n","\n","Epoch 00162: val_loss did not improve from 0.10516\n","\n","Epoch 00163: val_loss did not improve from 0.10516\n","\n","Epoch 00164: val_loss did not improve from 0.10516\n","\n","Epoch 00165: val_loss improved from 0.10516 to 0.10485, saving model to ./model/165-0.1049.hdf5\n","\n","Epoch 00166: val_loss improved from 0.10485 to 0.10311, saving model to ./model/166-0.1031.hdf5\n","\n","Epoch 00167: val_loss did not improve from 0.10311\n","\n","Epoch 00168: val_loss did not improve from 0.10311\n","\n","Epoch 00169: val_loss did not improve from 0.10311\n","\n","Epoch 00170: val_loss did not improve from 0.10311\n","\n","Epoch 00171: val_loss did not improve from 0.10311\n","\n","Epoch 00172: val_loss did not improve from 0.10311\n","\n","Epoch 00173: val_loss did not improve from 0.10311\n","\n","Epoch 00174: val_loss did not improve from 0.10311\n","\n","Epoch 00175: val_loss did not improve from 0.10311\n","\n","Epoch 00176: val_loss did not improve from 0.10311\n","\n","Epoch 00177: val_loss did not improve from 0.10311\n","\n","Epoch 00178: val_loss did not improve from 0.10311\n","\n","Epoch 00179: val_loss did not improve from 0.10311\n","\n","Epoch 00180: val_loss did not improve from 0.10311\n","\n","Epoch 00181: val_loss did not improve from 0.10311\n","\n","Epoch 00182: val_loss did not improve from 0.10311\n","\n","Epoch 00183: val_loss did not improve from 0.10311\n","\n","Epoch 00184: val_loss did not improve from 0.10311\n","\n","Epoch 00185: val_loss did not improve from 0.10311\n","\n","Epoch 00186: val_loss did not improve from 0.10311\n","\n","Epoch 00187: val_loss did not improve from 0.10311\n","\n","Epoch 00188: val_loss did not improve from 0.10311\n","\n","Epoch 00189: val_loss did not improve from 0.10311\n","\n","Epoch 00190: val_loss did not improve from 0.10311\n","\n","Epoch 00191: val_loss did not improve from 0.10311\n","\n","Epoch 00192: val_loss did not improve from 0.10311\n","\n","Epoch 00193: val_loss did not improve from 0.10311\n","\n","Epoch 00194: val_loss did not improve from 0.10311\n","\n","Epoch 00195: val_loss did not improve from 0.10311\n","\n","Epoch 00196: val_loss did not improve from 0.10311\n","\n","Epoch 00197: val_loss did not improve from 0.10311\n","\n","Epoch 00198: val_loss did not improve from 0.10311\n","\n","Epoch 00199: val_loss did not improve from 0.10311\n","\n","Epoch 00200: val_loss improved from 0.10311 to 0.10249, saving model to ./model/200-0.1025.hdf5\n","\n","Epoch 00201: val_loss did not improve from 0.10249\n","\n","Epoch 00202: val_loss did not improve from 0.10249\n","\n","Epoch 00203: val_loss did not improve from 0.10249\n","\n","Epoch 00204: val_loss did not improve from 0.10249\n","\n","Epoch 00205: val_loss did not improve from 0.10249\n","\n","Epoch 00206: val_loss did not improve from 0.10249\n","\n","Epoch 00207: val_loss did not improve from 0.10249\n","\n","Epoch 00208: val_loss did not improve from 0.10249\n","\n","Epoch 00209: val_loss did not improve from 0.10249\n","\n","Epoch 00210: val_loss did not improve from 0.10249\n","\n","Epoch 00211: val_loss did not improve from 0.10249\n","\n","Epoch 00212: val_loss did not improve from 0.10249\n","\n","Epoch 00213: val_loss did not improve from 0.10249\n","\n","Epoch 00214: val_loss improved from 0.10249 to 0.09927, saving model to ./model/214-0.0993.hdf5\n","\n","Epoch 00215: val_loss did not improve from 0.09927\n","\n","Epoch 00216: val_loss did not improve from 0.09927\n","\n","Epoch 00217: val_loss did not improve from 0.09927\n","\n","Epoch 00218: val_loss did not improve from 0.09927\n","\n","Epoch 00219: val_loss did not improve from 0.09927\n","\n","Epoch 00220: val_loss did not improve from 0.09927\n","\n","Epoch 00221: val_loss did not improve from 0.09927\n","\n","Epoch 00222: val_loss did not improve from 0.09927\n","\n","Epoch 00223: val_loss did not improve from 0.09927\n","\n","Epoch 00224: val_loss did not improve from 0.09927\n","\n","Epoch 00225: val_loss did not improve from 0.09927\n","\n","Epoch 00226: val_loss did not improve from 0.09927\n","\n","Epoch 00227: val_loss did not improve from 0.09927\n","\n","Epoch 00228: val_loss did not improve from 0.09927\n","\n","Epoch 00229: val_loss did not improve from 0.09927\n","\n","Epoch 00230: val_loss did not improve from 0.09927\n","\n","Epoch 00231: val_loss did not improve from 0.09927\n","\n","Epoch 00232: val_loss did not improve from 0.09927\n","\n","Epoch 00233: val_loss did not improve from 0.09927\n","\n","Epoch 00234: val_loss did not improve from 0.09927\n","\n","Epoch 00235: val_loss did not improve from 0.09927\n","\n","Epoch 00236: val_loss did not improve from 0.09927\n","\n","Epoch 00237: val_loss did not improve from 0.09927\n","\n","Epoch 00238: val_loss did not improve from 0.09927\n","\n","Epoch 00239: val_loss did not improve from 0.09927\n","\n","Epoch 00240: val_loss did not improve from 0.09927\n","\n","Epoch 00241: val_loss did not improve from 0.09927\n","\n","Epoch 00242: val_loss did not improve from 0.09927\n","\n","Epoch 00243: val_loss did not improve from 0.09927\n","\n","Epoch 00244: val_loss did not improve from 0.09927\n","\n","Epoch 00245: val_loss did not improve from 0.09927\n","\n","Epoch 00246: val_loss did not improve from 0.09927\n","\n","Epoch 00247: val_loss did not improve from 0.09927\n","\n","Epoch 00248: val_loss did not improve from 0.09927\n","\n","Epoch 00249: val_loss did not improve from 0.09927\n","\n","Epoch 00250: val_loss did not improve from 0.09927\n","\n","Epoch 00251: val_loss did not improve from 0.09927\n","\n","Epoch 00252: val_loss did not improve from 0.09927\n","\n","Epoch 00253: val_loss did not improve from 0.09927\n","\n","Epoch 00254: val_loss did not improve from 0.09927\n","\n","Epoch 00255: val_loss did not improve from 0.09927\n","\n","Epoch 00256: val_loss did not improve from 0.09927\n","\n","Epoch 00257: val_loss did not improve from 0.09927\n","\n","Epoch 00258: val_loss did not improve from 0.09927\n","\n","Epoch 00259: val_loss did not improve from 0.09927\n","\n","Epoch 00260: val_loss did not improve from 0.09927\n","\n","Epoch 00261: val_loss did not improve from 0.09927\n","\n","Epoch 00262: val_loss did not improve from 0.09927\n","\n","Epoch 00263: val_loss did not improve from 0.09927\n","\n","Epoch 00264: val_loss did not improve from 0.09927\n","\n","Epoch 00265: val_loss did not improve from 0.09927\n","\n","Epoch 00266: val_loss did not improve from 0.09927\n","\n","Epoch 00267: val_loss did not improve from 0.09927\n","\n","Epoch 00268: val_loss did not improve from 0.09927\n","\n","Epoch 00269: val_loss did not improve from 0.09927\n","\n","Epoch 00270: val_loss did not improve from 0.09927\n","\n","Epoch 00271: val_loss did not improve from 0.09927\n","\n","Epoch 00272: val_loss did not improve from 0.09927\n","\n","Epoch 00273: val_loss did not improve from 0.09927\n","\n","Epoch 00274: val_loss did not improve from 0.09927\n","\n","Epoch 00275: val_loss did not improve from 0.09927\n","\n","Epoch 00276: val_loss did not improve from 0.09927\n","\n","Epoch 00277: val_loss did not improve from 0.09927\n","\n","Epoch 00278: val_loss did not improve from 0.09927\n","\n","Epoch 00279: val_loss did not improve from 0.09927\n","\n","Epoch 00280: val_loss did not improve from 0.09927\n","\n","Epoch 00281: val_loss did not improve from 0.09927\n","\n","Epoch 00282: val_loss did not improve from 0.09927\n","\n","Epoch 00283: val_loss did not improve from 0.09927\n","\n","Epoch 00284: val_loss did not improve from 0.09927\n","\n","Epoch 00285: val_loss did not improve from 0.09927\n","\n","Epoch 00286: val_loss did not improve from 0.09927\n","\n","Epoch 00287: val_loss did not improve from 0.09927\n","\n","Epoch 00288: val_loss did not improve from 0.09927\n","\n","Epoch 00289: val_loss did not improve from 0.09927\n","\n","Epoch 00290: val_loss did not improve from 0.09927\n","\n","Epoch 00291: val_loss did not improve from 0.09927\n","\n","Epoch 00292: val_loss did not improve from 0.09927\n","\n","Epoch 00293: val_loss did not improve from 0.09927\n","\n","Epoch 00294: val_loss did not improve from 0.09927\n","\n","Epoch 00295: val_loss did not improve from 0.09927\n","\n","Epoch 00296: val_loss did not improve from 0.09927\n","\n","Epoch 00297: val_loss did not improve from 0.09927\n","\n","Epoch 00298: val_loss did not improve from 0.09927\n","\n","Epoch 00299: val_loss did not improve from 0.09927\n","\n","Epoch 00300: val_loss did not improve from 0.09927\n","\n","Epoch 00301: val_loss did not improve from 0.09927\n","\n","Epoch 00302: val_loss did not improve from 0.09927\n","\n","Epoch 00303: val_loss did not improve from 0.09927\n","\n","Epoch 00304: val_loss did not improve from 0.09927\n","\n","Epoch 00305: val_loss did not improve from 0.09927\n","\n","Epoch 00306: val_loss did not improve from 0.09927\n","\n","Epoch 00307: val_loss did not improve from 0.09927\n","\n","Epoch 00308: val_loss did not improve from 0.09927\n","\n","Epoch 00309: val_loss did not improve from 0.09927\n","\n","Epoch 00310: val_loss did not improve from 0.09927\n","\n","Epoch 00311: val_loss did not improve from 0.09927\n","\n","Epoch 00312: val_loss did not improve from 0.09927\n","\n","Epoch 00313: val_loss did not improve from 0.09927\n","\n","Epoch 00314: val_loss did not improve from 0.09927\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"vZkNuqKw_CzP","executionInfo":{"status":"ok","timestamp":1605355347359,"user_tz":-540,"elapsed":948,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"58d39cdb-0eda-46de-c8a0-8494cbc352eb","colab":{"base_uri":"https://localhost:8080/"}},"source":["history.history.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"39jtzRIR7uzy","executionInfo":{"status":"ok","timestamp":1605355349787,"user_tz":-540,"elapsed":729,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"491d57ff-120b-45ed-e3db-36498b958738","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"Acuracy: %.4f\" %(model.evaluate(X, y)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["41/41 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9823\n","Acuracy: 0.9823\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"meV-aZzQ7uz3"},"source":["- 2000 epoch 전에 중간에 중단됨을 알 수 있다."]},{"cell_type":"markdown","metadata":{"id":"jQg_oBsA7uz4"},"source":["## 15. Linear Regression"]},{"cell_type":"code","metadata":{"id":"cB-XZLfh7uz5","executionInfo":{"status":"ok","timestamp":1605355436335,"user_tz":-540,"elapsed":656,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"e3e3184b-492a-47e8-b008-a45faeb122aa","colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","\n","import numpy\n","import pandas as pd\n","import tensorflow as tf\n","\n","df = pd.read_csv(\"boston_housing.csv\", delim_whitespace=True, header=None)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0     1     2   3      4      5   ...  8      9     10      11    12    13\n","0  0.00632  18.0  2.31   0  0.538  6.575  ...   1  296.0  15.3  396.90  4.98  24.0\n","1  0.02731   0.0  7.07   0  0.469  6.421  ...   2  242.0  17.8  396.90  9.14  21.6\n","2  0.02729   0.0  7.07   0  0.469  7.185  ...   2  242.0  17.8  392.83  4.03  34.7\n","3  0.03237   0.0  2.18   0  0.458  6.998  ...   3  222.0  18.7  394.63  2.94  33.4\n","4  0.06905   0.0  2.18   0  0.458  7.147  ...   3  222.0  18.7  396.90  5.33  36.2\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"A7YEdioL7uz-","executionInfo":{"status":"ok","timestamp":1605355444713,"user_tz":-540,"elapsed":823,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"9aad7788-97b5-4e92-f4f1-fb3106fd0b3c","colab":{"base_uri":"https://localhost:8080/"}},"source":["df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 506 entries, 0 to 505\n","Data columns (total 14 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   0       506 non-null    float64\n"," 1   1       506 non-null    float64\n"," 2   2       506 non-null    float64\n"," 3   3       506 non-null    int64  \n"," 4   4       506 non-null    float64\n"," 5   5       506 non-null    float64\n"," 6   6       506 non-null    float64\n"," 7   7       506 non-null    float64\n"," 8   8       506 non-null    int64  \n"," 9   9       506 non-null    float64\n"," 10  10      506 non-null    float64\n"," 11  11      506 non-null    float64\n"," 12  12      506 non-null    float64\n"," 13  13      506 non-null    float64\n","dtypes: float64(12), int64(2)\n","memory usage: 55.5 KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WzE3FKCa7u0F","executionInfo":{"status":"ok","timestamp":1605355514457,"user_tz":-540,"elapsed":16281,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"a04916cc-885c-48c6-daed-f805e172f263","colab":{"base_uri":"https://localhost:8080/"}},"source":["dataset = df.values\n","X, y = dataset[:,0:13] , dataset[:,13]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","model = Sequential()\n","model.add(Dense(30, input_dim=13, activation='relu'))\n","model.add(Dense(6, activation='relu'))\n","model.add(Dense(1))    # no need of activation function\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='adam')\n","\n","model.fit(X_train, y_train, epochs=200, batch_size=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","36/36 [==============================] - 0s 2ms/step - loss: 2037.9723\n","Epoch 2/200\n","36/36 [==============================] - 0s 2ms/step - loss: 257.1145\n","Epoch 3/200\n","36/36 [==============================] - 0s 2ms/step - loss: 155.9259\n","Epoch 4/200\n","36/36 [==============================] - 0s 2ms/step - loss: 114.3591\n","Epoch 5/200\n","36/36 [==============================] - 0s 2ms/step - loss: 86.0686\n","Epoch 6/200\n","36/36 [==============================] - 0s 2ms/step - loss: 74.1227\n","Epoch 7/200\n","36/36 [==============================] - 0s 2ms/step - loss: 66.5325\n","Epoch 8/200\n","36/36 [==============================] - 0s 2ms/step - loss: 62.4586\n","Epoch 9/200\n","36/36 [==============================] - 0s 2ms/step - loss: 60.1356\n","Epoch 10/200\n","36/36 [==============================] - 0s 2ms/step - loss: 59.0248\n","Epoch 11/200\n","36/36 [==============================] - 0s 2ms/step - loss: 56.2528\n","Epoch 12/200\n","36/36 [==============================] - 0s 2ms/step - loss: 52.0518\n","Epoch 13/200\n","36/36 [==============================] - 0s 2ms/step - loss: 52.0495\n","Epoch 14/200\n","36/36 [==============================] - 0s 2ms/step - loss: 52.6548\n","Epoch 15/200\n","36/36 [==============================] - 0s 2ms/step - loss: 49.8636\n","Epoch 16/200\n","36/36 [==============================] - 0s 2ms/step - loss: 48.4561\n","Epoch 17/200\n","36/36 [==============================] - 0s 2ms/step - loss: 47.5955\n","Epoch 18/200\n","36/36 [==============================] - 0s 2ms/step - loss: 51.4236\n","Epoch 19/200\n","36/36 [==============================] - 0s 2ms/step - loss: 45.7349\n","Epoch 20/200\n","36/36 [==============================] - 0s 2ms/step - loss: 47.5018\n","Epoch 21/200\n","36/36 [==============================] - 0s 2ms/step - loss: 45.2360\n","Epoch 22/200\n","36/36 [==============================] - 0s 2ms/step - loss: 46.3777\n","Epoch 23/200\n","36/36 [==============================] - 0s 2ms/step - loss: 43.4743\n","Epoch 24/200\n","36/36 [==============================] - 0s 2ms/step - loss: 43.6965\n","Epoch 25/200\n","36/36 [==============================] - 0s 2ms/step - loss: 43.6133\n","Epoch 26/200\n","36/36 [==============================] - 0s 2ms/step - loss: 42.9763\n","Epoch 27/200\n","36/36 [==============================] - 0s 2ms/step - loss: 42.8318\n","Epoch 28/200\n","36/36 [==============================] - 0s 2ms/step - loss: 41.3480\n","Epoch 29/200\n","36/36 [==============================] - 0s 2ms/step - loss: 42.9277\n","Epoch 30/200\n","36/36 [==============================] - 0s 2ms/step - loss: 43.4493\n","Epoch 31/200\n","36/36 [==============================] - 0s 2ms/step - loss: 40.0936\n","Epoch 32/200\n","36/36 [==============================] - 0s 2ms/step - loss: 40.0701\n","Epoch 33/200\n","36/36 [==============================] - 0s 2ms/step - loss: 42.0273\n","Epoch 34/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.7973\n","Epoch 35/200\n","36/36 [==============================] - 0s 2ms/step - loss: 39.4386\n","Epoch 36/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.8064\n","Epoch 37/200\n","36/36 [==============================] - 0s 2ms/step - loss: 41.1691\n","Epoch 38/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.1925\n","Epoch 39/200\n","36/36 [==============================] - 0s 2ms/step - loss: 47.1418\n","Epoch 40/200\n","36/36 [==============================] - 0s 2ms/step - loss: 40.3293\n","Epoch 41/200\n","36/36 [==============================] - 0s 2ms/step - loss: 37.1798\n","Epoch 42/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.9911\n","Epoch 43/200\n","36/36 [==============================] - 0s 2ms/step - loss: 37.4873\n","Epoch 44/200\n","36/36 [==============================] - 0s 2ms/step - loss: 37.4341\n","Epoch 45/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.9559\n","Epoch 46/200\n","36/36 [==============================] - 0s 2ms/step - loss: 37.8185\n","Epoch 47/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.6153\n","Epoch 48/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.3457\n","Epoch 49/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.5648\n","Epoch 50/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.8600\n","Epoch 51/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.6471\n","Epoch 52/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.7502\n","Epoch 53/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.5262\n","Epoch 54/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.6102\n","Epoch 55/200\n","36/36 [==============================] - 0s 2ms/step - loss: 39.0586\n","Epoch 56/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.2094\n","Epoch 57/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.6028\n","Epoch 58/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.9260\n","Epoch 59/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.3808\n","Epoch 60/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.5574\n","Epoch 61/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.3832\n","Epoch 62/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.4944\n","Epoch 63/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.3652\n","Epoch 64/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.9118\n","Epoch 65/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.1993\n","Epoch 66/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.5356\n","Epoch 67/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.5115\n","Epoch 68/200\n","36/36 [==============================] - 0s 2ms/step - loss: 44.2310\n","Epoch 69/200\n","36/36 [==============================] - 0s 2ms/step - loss: 40.1809\n","Epoch 70/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.0500\n","Epoch 71/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.2910\n","Epoch 72/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.9680\n","Epoch 73/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.7622\n","Epoch 74/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.0981\n","Epoch 75/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.7007\n","Epoch 76/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.4031\n","Epoch 77/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.1842\n","Epoch 78/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.6076\n","Epoch 79/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.5176\n","Epoch 80/200\n","36/36 [==============================] - 0s 2ms/step - loss: 38.3730\n","Epoch 81/200\n","36/36 [==============================] - 0s 2ms/step - loss: 41.4277\n","Epoch 82/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.4181\n","Epoch 83/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.2702\n","Epoch 84/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.5387\n","Epoch 85/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.5137\n","Epoch 86/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.4671\n","Epoch 87/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.1579\n","Epoch 88/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.5804\n","Epoch 89/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.0061\n","Epoch 90/200\n","36/36 [==============================] - 0s 2ms/step - loss: 35.0871\n","Epoch 91/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.8466\n","Epoch 92/200\n","36/36 [==============================] - 0s 2ms/step - loss: 31.7626\n","Epoch 93/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.0849\n","Epoch 94/200\n","36/36 [==============================] - 0s 2ms/step - loss: 31.9930\n","Epoch 95/200\n","36/36 [==============================] - 0s 2ms/step - loss: 37.9778\n","Epoch 96/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.0248\n","Epoch 97/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.6789\n","Epoch 98/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.0548\n","Epoch 99/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.6218\n","Epoch 100/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.6554\n","Epoch 101/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.5750\n","Epoch 102/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.6782\n","Epoch 103/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.6956\n","Epoch 104/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.2870\n","Epoch 105/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.8790\n","Epoch 106/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.8827\n","Epoch 107/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.1707\n","Epoch 108/200\n","36/36 [==============================] - 0s 2ms/step - loss: 34.0393\n","Epoch 109/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.1282\n","Epoch 110/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.0070\n","Epoch 111/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.7256\n","Epoch 112/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.3096\n","Epoch 113/200\n","36/36 [==============================] - 0s 2ms/step - loss: 36.4512\n","Epoch 114/200\n","36/36 [==============================] - 0s 2ms/step - loss: 31.4712\n","Epoch 115/200\n","36/36 [==============================] - 0s 2ms/step - loss: 32.2841\n","Epoch 116/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.7014\n","Epoch 117/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.9336\n","Epoch 118/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.9374\n","Epoch 119/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.6437\n","Epoch 120/200\n","36/36 [==============================] - 0s 2ms/step - loss: 33.5385\n","Epoch 121/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.4022\n","Epoch 122/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.7258\n","Epoch 123/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.7647\n","Epoch 124/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.3388\n","Epoch 125/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.8715\n","Epoch 126/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.0046\n","Epoch 127/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.0560\n","Epoch 128/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.7448\n","Epoch 129/200\n","36/36 [==============================] - 0s 2ms/step - loss: 29.2606\n","Epoch 130/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.6252\n","Epoch 131/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.9669\n","Epoch 132/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.2238\n","Epoch 133/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.2838\n","Epoch 134/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.2070\n","Epoch 135/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.5731\n","Epoch 136/200\n","36/36 [==============================] - 0s 2ms/step - loss: 28.1782\n","Epoch 137/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.1632\n","Epoch 138/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.3006\n","Epoch 139/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.8287\n","Epoch 140/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.8375\n","Epoch 141/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.7722\n","Epoch 142/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.4735\n","Epoch 143/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.5459\n","Epoch 144/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.7347\n","Epoch 145/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.9658\n","Epoch 146/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.3498\n","Epoch 147/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.1528\n","Epoch 148/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.1415\n","Epoch 149/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.5905\n","Epoch 150/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.9180\n","Epoch 151/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.3244\n","Epoch 152/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.4533\n","Epoch 153/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.7801\n","Epoch 154/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.9932\n","Epoch 155/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.5385\n","Epoch 156/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.2874\n","Epoch 157/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.0017\n","Epoch 158/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.4757\n","Epoch 159/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.6595\n","Epoch 160/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.3020\n","Epoch 161/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.5888\n","Epoch 162/200\n","36/36 [==============================] - 0s 2ms/step - loss: 24.2746\n","Epoch 163/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.9487\n","Epoch 164/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.1285\n","Epoch 165/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.0394\n","Epoch 166/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.8353\n","Epoch 167/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.8372\n","Epoch 168/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.4088\n","Epoch 169/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.1547\n","Epoch 170/200\n","36/36 [==============================] - 0s 2ms/step - loss: 31.7647\n","Epoch 171/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.6858\n","Epoch 172/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.0503\n","Epoch 173/200\n","36/36 [==============================] - 0s 2ms/step - loss: 21.2414\n","Epoch 174/200\n","36/36 [==============================] - 0s 2ms/step - loss: 21.3057\n","Epoch 175/200\n","36/36 [==============================] - 0s 2ms/step - loss: 20.7954\n","Epoch 176/200\n","36/36 [==============================] - 0s 2ms/step - loss: 27.1219\n","Epoch 177/200\n","36/36 [==============================] - 0s 2ms/step - loss: 30.3974\n","Epoch 178/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.8553\n","Epoch 179/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.8319\n","Epoch 180/200\n","36/36 [==============================] - 0s 2ms/step - loss: 19.2305\n","Epoch 181/200\n","36/36 [==============================] - 0s 2ms/step - loss: 20.9937\n","Epoch 182/200\n","36/36 [==============================] - 0s 2ms/step - loss: 25.7125\n","Epoch 183/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.8542\n","Epoch 184/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.8183\n","Epoch 185/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.2713\n","Epoch 186/200\n","36/36 [==============================] - 0s 2ms/step - loss: 22.4605\n","Epoch 187/200\n","36/36 [==============================] - 0s 2ms/step - loss: 20.4462\n","Epoch 188/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.2153\n","Epoch 189/200\n","36/36 [==============================] - 0s 2ms/step - loss: 19.5257\n","Epoch 190/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.9471\n","Epoch 191/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.0818\n","Epoch 192/200\n","36/36 [==============================] - 0s 2ms/step - loss: 19.7382\n","Epoch 193/200\n","36/36 [==============================] - 0s 2ms/step - loss: 21.8955\n","Epoch 194/200\n","36/36 [==============================] - 0s 2ms/step - loss: 21.9266\n","Epoch 195/200\n","36/36 [==============================] - 0s 2ms/step - loss: 20.0089\n","Epoch 196/200\n","36/36 [==============================] - 0s 2ms/step - loss: 18.5975\n","Epoch 197/200\n","36/36 [==============================] - 0s 2ms/step - loss: 19.9648\n","Epoch 198/200\n","36/36 [==============================] - 0s 2ms/step - loss: 19.3643\n","Epoch 199/200\n","36/36 [==============================] - 0s 2ms/step - loss: 26.7769\n","Epoch 200/200\n","36/36 [==============================] - 0s 2ms/step - loss: 23.5552\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f83f1dc6208>"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"XDOd_6B47u0L","executionInfo":{"status":"ok","timestamp":1605355549746,"user_tz":-540,"elapsed":784,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"9056ab9b-b1ac-45ff-a9b2-7f03bc8166da","colab":{"base_uri":"https://localhost:8080/"}},"source":["model.predict(X_test)[:5][:,0] == model.predict(X_test)[:5].flatten()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True,  True,  True,  True,  True])"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"3cUW5F8m7u0S","executionInfo":{"status":"ok","timestamp":1605355558862,"user_tz":-540,"elapsed":718,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"8e9b81a3-994d-4e52-9ac4-adc69386d02c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# 예측 값과 실제 값의 비교\n","y_pred = model.predict(X_test).flatten()\n","for i in range(10):\n","    label = y_test[i]\n","    prediction = y_pred[i]\n","    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["실제가격: 28.100, 예상가격: 24.431\n","실제가격: 19.500, 예상가격: 23.639\n","실제가격: 20.000, 예상가격: 23.076\n","실제가격: 35.200, 예상가격: 33.727\n","실제가격: 27.900, 예상가격: 36.100\n","실제가격: 22.100, 예상가격: 30.902\n","실제가격: 8.800, 예상가격: 15.666\n","실제가격: 36.000, 예상가격: 35.070\n","실제가격: 17.100, 예상가격: 17.970\n","실제가격: 25.100, 예상가격: 32.036\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4xpM5TEGZSy_"},"source":["# Exercise (연습)"]},{"cell_type":"code","metadata":{"id":"povINYYN7u0Y","executionInfo":{"status":"ok","timestamp":1605353676285,"user_tz":-540,"elapsed":716,"user":{"displayName":"Yongjin Jeong","photoUrl":"","userId":"03658406798560557048"}},"outputId":"d51ea585-7cbf-4d03-9c27-9fc95528dfd4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# tf.function: Compiles a function into a callable TensorFlow graph.\n","#               (must be faster), but not always...?\n","import timeit\n","@tf.function\n","def dense_layer1(x, w, b):\n","    return tf.add(tf.matmul(x, w), b)\n","\n","def dense_layer2(x, w, b):\n","    return tf.add(tf.matmul(x, w), b)\n","  \n","%time dense_layer1(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))\n","print()\n","%time dense_layer2(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))\n","print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 12.2 ms, sys: 0 ns, total: 12.2 ms\n","Wall time: 13.8 ms\n","\n","CPU times: user 1.44 ms, sys: 0 ns, total: 1.44 ms\n","Wall time: 1.08 ms\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u8XV1hi7bX5-"},"source":["import timeit\n","conv_layer = tf.keras.layers.Conv2D(100, 3)\n","\n","@tf.function\n","def conv_fn(image):\n","  return conv_layer(image)\n","\n","image = tf.zeros([1, 200, 200, 100])\n","# 워밍 업\n","conv_layer(image); conv_fn(image)\n","print(\"즉시 실행 합성곱:\", timeit.timeit(lambda: conv_layer(image), number=10))\n","print(\"tf.function 합성곱:\", timeit.timeit(lambda: conv_fn(image), number=10))\n","print(\"합성곱 연산 속도에 큰 차이가 없습니다.\")"],"execution_count":null,"outputs":[]}]}